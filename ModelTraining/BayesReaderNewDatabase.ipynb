{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_df, spamProbality = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TestPhraseList(phraseList,spamProba=1):\n",
    "    for word in phraseList:\n",
    "        vraisemblance = loaded_df[\"vraisemblance\"].get(word,\"NotFound\")\n",
    "        if vraisemblance!=\"NotFound\":\n",
    "            spamProba*=vraisemblance\n",
    "    return(spamProba)\n",
    "\n",
    "def TestPhrase(phrase:str,spamProba=1):\n",
    "    phraseList=phrase.split()\n",
    "    return TestPhraseList(phraseList,spamProba=spamProba)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDF = pd.read_csv(\n",
    "    \"./DataSetBrut/DataSmsSpamNH.csv\", delimiter=\"\\t\", header=None, names=[\"spam\", \"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDF[\"text\"]=testDF[\"text\"].str.split()\n",
    "testDF[\"spam\"]=testDF[\"spam\"].apply(str.lower)\n",
    "testDF[\"result\"]=testDF[\"text\"].apply(TestPhraseList,spamProba=spamProbality)\n",
    "testDF[\"Finalresult\"]=['spam' if x > 1 else 'ham' for x in testDF['result']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exactitude : 0.7008894536213469\n",
      "Rappel : 0.7243090007087172\n",
      "Rappel négative: 0.6415094339622641\n",
      "Precision : 0.836676217765043\n",
      "Précision négative: 0.4785522788203753\n",
      "F1-Score : 0.7764482431149098\n",
      "F1-Score Négatif: 0.9986979384583459\n"
     ]
    }
   ],
   "source": [
    "wrong = len(testDF[testDF[\"Finalresult\"]!=testDF[\"spam\"]].index)\n",
    "FP = len(testDF[(testDF[\"Finalresult\"]==\"spam\")&(testDF[\"spam\"]==\"ham\")].index)\n",
    "FN = len(testDF[(testDF[\"Finalresult\"]==\"ham\")&(testDF[\"spam\"]==\"spam\")].index)\n",
    "VP = len(testDF[(testDF[\"Finalresult\"]==\"ham\")&(testDF[\"spam\"]==\"ham\")].index)\n",
    "VN = len(testDF[(testDF[\"Finalresult\"]==\"spam\")&(testDF[\"spam\"]==\"spam\")].index)\n",
    "wrong/len(testDF.index)\n",
    "exactitude = (VN+VP)/(VN+FN+FP+VP)\n",
    "print(f\"Exactitude : {exactitude}\")\n",
    "rappel = VP /(VP+FN)\n",
    "print(f\"Rappel : {rappel}\")\n",
    "print(f\"Rappel négative: {VN/(VN+FP)}\")\n",
    "precision = VP/(VP+FP)\n",
    "print(f\"Precision : {precision}\")\n",
    "print(f\"Précision négative: {VN/(VN+FN)}\")\n",
    "F1Score = (2 * precision * rappel) / (precision + rappel)\n",
    "print(f\"F1-Score : {F1Score}\")\n",
    "print(f\"F1-Score Négatif: {(2 * VN/(VN+FN) * VN/VN+FP )/ (VN/(VN+FN) + VN/VN+FP)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
