{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parser\n",
    "parser = argparse.ArgumentParser(prog=\"BayesienReader\",description=\"Creer un modele bayesien avec une base de donnée sous format csv séparer par tabulation en premier l'étiquette 'ham' ou 'spam' et le contenue du sms\")\n",
    "\n",
    "# Add arguments\n",
    "parser.add_argument('--TFile', type=str, help=\"The path to the training data\", required=True)\n",
    "parser.add_argument('--OFile',type = str,help=\"The name of the outputfile\")\n",
    "# Parse arguments\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_df, spamProbality = joblib.load('BayesienModel.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TestPhraseList(phraseList,spamProba=1):\n",
    "    for word in phraseList:\n",
    "        vraisemblance = loaded_df[\"vraisemblance\"].get(word,\"NotFound\")\n",
    "        if vraisemblance!=\"NotFound\":\n",
    "            spamProba*=vraisemblance\n",
    "    return(spamProba)\n",
    "\n",
    "def TestPhrase(phrase:str,spamProba=1):\n",
    "    phraseList=phrase.split()\n",
    "    return TestPhraseList(phraseList,spamProba=spamProba)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDF = pd.read_csv(\n",
    "    \"SMSSpamCollection.txt\", delimiter=\"\\t\", header=None, names=[\"spam\", \"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDF[\"text\"]=testDF[\"text\"].str.split()\n",
    "testDF[\"result\"]=testDF[\"text\"].apply(TestPhraseList,spamProba=spamProbality)\n",
    "testDF[\"Finalresult\"]=['spam' if x > 1 else 'ham' for x in testDF['result']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exactitude : 0.9800789662598708\n",
      "Rappel : 0.9972573839662447\n",
      "Precision : 0.9796891191709844\n",
      "F1-Score : 0.988395190799791\n"
     ]
    }
   ],
   "source": [
    "wrong = len(testDF[testDF[\"Finalresult\"]!=testDF[\"spam\"]].index)\n",
    "FP = len(testDF[(testDF[\"Finalresult\"]==\"spam\")&(testDF[\"spam\"]==\"ham\")].index)\n",
    "FN = len(testDF[(testDF[\"Finalresult\"]==\"ham\")&(testDF[\"spam\"]==\"spam\")].index)\n",
    "VP = len(testDF[(testDF[\"Finalresult\"]==\"ham\")&(testDF[\"spam\"]==\"ham\")].index)\n",
    "VN = len(testDF[(testDF[\"Finalresult\"]==\"spam\")&(testDF[\"spam\"]==\"spam\")].index)\n",
    "wrong/len(testDF.index)\n",
    "exactitude = (VN+VP)/(VN+FN+FP+VP)\n",
    "print(f\"Exactitude : {exactitude}\")\n",
    "rappel = VP /(VP+FN)\n",
    "print(f\"Rappel : {rappel}\")\n",
    "precision = VP/(VP+FP)\n",
    "print(f\"Precision : {precision}\")\n",
    "F1Score = (2 * precision * rappel) / (precision + rappel)\n",
    "print(f\"F1-Score : {F1Score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
